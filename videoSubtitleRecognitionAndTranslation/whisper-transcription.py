import whisper
import json
import os
import time
import warnings
from pathlib import Path
from typing import List, Dict, Optional
import numpy as np
import librosa
import gc
import psutil
import subprocess
import tempfile
import math
import wave
import struct
from datetime import timedelta

warnings.filterwarnings("ignore")


class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""
    
    def __init__(self):
        self.process = psutil.Process()
    
    def get_memory_usage_mb(self) -> float:
        """è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰"""
        return self.process.memory_info().rss / 1024 / 1024
    
    def get_available_memory_mb(self) -> float:
        """è·å–ç³»ç»Ÿå¯ç”¨å†…å­˜ï¼ˆMBï¼‰"""
        return psutil.virtual_memory().available / 1024 / 1024
    
    def get_total_memory_mb(self) -> float:
        """è·å–ç³»ç»Ÿæ€»å†…å­˜ï¼ˆMBï¼‰"""
        return psutil.virtual_memory().total / 1024 / 1024
    
    def log_memory_usage(self, stage: str = ""):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        used = self.get_memory_usage_mb()
        available = self.get_available_memory_mb()
        total = self.get_total_memory_mb()
        
        print(f"å†…å­˜ä½¿ç”¨[{stage}]: è¿›ç¨‹{used:.0f}MB, "
              f"ç³»ç»Ÿå¯ç”¨{available:.0f}MB/{total:.0f}MB")


class MemoryEfficientWhisper:
    """
    å†…å­˜é«˜æ•ˆçš„Whisperè½¬å½•å™¨ï¼Œä¸“ä¸ºæœ‰é™å†…å­˜ç¯å¢ƒè®¾è®¡
    æ”¯æŒlarge-v2æ¨¡å‹åœ¨16GBå†…å­˜æœºå™¨ä¸Šè¿è¡Œ
    """
    
    def __init__(self, model_or_size="large-v2", device="cpu", 
                 max_chunk_duration=60,  # é»˜è®¤60ç§’ä¸€å—ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
                 checkpoint_dir="memory_safe_checkpoints"):
        """
        åˆå§‹åŒ–å†…å­˜é«˜æ•ˆè½¬å½•å™¨
        
        Args:
            model_or_size: æ¨¡å‹å¤§å°æˆ–å·²åŠ è½½çš„æ¨¡å‹å¯¹è±¡ï¼Œå¯¹äº16GBå†…å­˜æ¨è"large-v2"
            device: è¿è¡Œè®¾å¤‡ï¼Œ16GBå†…å­˜å»ºè®®ç”¨cpu
            max_chunk_duration: æœ€å¤§åˆ†å—æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œæ ¹æ®å†…å­˜è°ƒæ•´
            checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        """
        # æ£€æŸ¥ä¼ å…¥çš„æ˜¯æ¨¡å‹å¯¹è±¡è¿˜æ˜¯æ¨¡å‹åç§°
        if isinstance(model_or_size, str):
            # ä¼ å…¥çš„æ˜¯æ¨¡å‹åç§°ï¼Œéœ€è¦åŠ è½½æ¨¡å‹
            print(f"åŠ è½½ {model_or_size} æ¨¡å‹...")
            self.model = whisper.load_model(model_or_size, device=device)
        else:
            # ä¼ å…¥çš„æ˜¯å·²åŠ è½½çš„æ¨¡å‹å¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
            print(f"å†…å­˜ä¼˜åŒ–è½¬å½•å™¨é‡ç”¨å·²åŠ è½½çš„æ¨¡å‹")
            self.model = model_or_size
            
        self.device = device
        self.max_chunk_duration = max_chunk_duration
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # ç›‘æ§å†…å­˜ä½¿ç”¨
        self.memory_monitor = MemoryMonitor()
        
        print(f"å†…å­˜ä¼˜åŒ–è½¬å½•å™¨åˆå§‹åŒ–å®Œæˆã€‚è®¾å¤‡: {device}")
        print(f"æœ€å¤§åˆ†å—æ—¶é•¿: {max_chunk_duration}ç§’")
        
    def get_checkpoint_path(self, audio_path: str) -> Path:
        """è·å–æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„"""
        audio_name = Path(audio_path).stem
        return self.checkpoint_dir / f"{audio_name}_memory_safe_checkpoint.json"
    
    def optimize_chunk_size(self, audio_duration: float, available_memory_mb: float) -> int:
        """
        æ ¹æ®éŸ³é¢‘é•¿åº¦å’Œå¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´åˆ†å—å¤§å°
        
        Args:
            audio_duration: éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
            available_memory_mb: å¯ç”¨å†…å­˜ï¼ˆMBï¼‰
            
        Returns:
            ä¼˜åŒ–çš„åˆ†å—æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        # large-v2æ¨¡å‹æœ¬èº«éœ€è¦çº¦4000MB
        model_memory = 4000
        
        # æ¯60ç§’éŸ³é¢‘å¤§çº¦éœ€è¦100MBå¤„ç†å†…å­˜
        memory_per_minute = 100 * 1024 / 60  # è½¬æ¢ä¸ºæ¯ç§’
        
        # è®¡ç®—å®‰å…¨å†…å­˜
        safe_memory = available_memory_mb - model_memory - 2000  # ä¿ç•™2GBç»™ç³»ç»Ÿ
        
        if safe_memory <= 0:
            return 30  # æœ€å°åˆ†å—
        
        # è®¡ç®—ç†è®ºæœ€å¤§åˆ†å—æ—¶é•¿
        max_possible = safe_memory / memory_per_minute
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        chunk_size = min(self.max_chunk_duration, max_possible)
        chunk_size = max(30, chunk_size)  # è‡³å°‘30ç§’
        
        # å¦‚æœéŸ³é¢‘å¾ˆçŸ­ï¼Œä¸éœ€è¦åˆ†å—
        if audio_duration <= 120:  # 2åˆ†é’Ÿä»¥å†…
            chunk_size = audio_duration
        
        print(f"å†…å­˜ä¼˜åŒ–: å¯ç”¨{available_memory_mb:.0f}MB, "
              f"å»ºè®®åˆ†å—{chunk_size:.0f}ç§’")
        
        return int(chunk_size)
    
    def transcribe_with_memory_safety(self, audio_path: str, **transcribe_kwargs) -> Dict:
        """
        å†…å­˜å®‰å…¨çš„è½¬å½•æ–¹æ³•ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            **transcribe_kwargs: Whisperè½¬å½•å‚æ•°
            
        Returns:
            è½¬å½•ç»“æœ
        """
        print(f"\nå¼€å§‹å†…å­˜å®‰å…¨è½¬å½•: {Path(audio_path).name}")
        
        # è·å–éŸ³é¢‘ä¿¡æ¯
        audio_duration = self.get_audio_duration(audio_path)
        available_memory = self.memory_monitor.get_available_memory_mb()
        
        # ä¼˜åŒ–åˆ†å—å¤§å°
        chunk_duration = self.optimize_chunk_size(audio_duration, available_memory)
        
        # æ£€æŸ¥ç‚¹è·¯å¾„
        checkpoint_path = self.get_checkpoint_path(audio_path)
        
        # å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
        if checkpoint_path.exists():
            print(f"å‘ç°å†…å­˜å®‰å…¨æ£€æŸ¥ç‚¹ï¼Œå°è¯•æ¢å¤...")
            return self.resume_from_checkpoint(audio_path, checkpoint_path, 
                                              chunk_duration, **transcribe_kwargs)
        else:
            print(f"å¼€å§‹æ–°çš„å†…å­˜å®‰å…¨è½¬å½•ï¼Œæ€»æ—¶é•¿: {audio_duration:.1f}ç§’")
            return self.new_transcription(audio_path, checkpoint_path,
                                         chunk_duration, **transcribe_kwargs)
    
    def get_audio_duration(self, audio_path: str) -> float:
        """è·å–éŸ³é¢‘æ—¶é•¿"""
        try:
            import soundfile as sf
            info = sf.info(audio_path)
            return info.duration
        except:
            # å¤‡ç”¨æ–¹æ³•
            audio, sr = librosa.load(audio_path, sr=None)
            return len(audio) / sr
    
    def new_transcription(self, audio_path: str, checkpoint_path: Path,
                         chunk_duration: int, **transcribe_kwargs) -> Dict:
        """å¼€å§‹æ–°çš„è½¬å½•"""
        # åˆå§‹åŒ–æ£€æŸ¥ç‚¹
        checkpoint = {
            "audio_path": audio_path,
            "chunk_duration": chunk_duration,
            "processed_chunks": [],
            "results": [],
            "current_chunk": 0,
            "total_chunks": 0,
            "transcribe_kwargs": transcribe_kwargs,
            "start_time": time.time()
        }
        
        # åŠ è½½éŸ³é¢‘
        print("åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
        audio, sr = librosa.load(audio_path, sr=16000)
        total_samples = len(audio)
        
        # è®¡ç®—åˆ†å—
        chunk_samples = chunk_duration * sr
        total_chunks = int(np.ceil(total_samples / chunk_samples))
        checkpoint["total_chunks"] = total_chunks
        
        print(f"éŸ³é¢‘æ€»é•¿åº¦: {total_samples/sr:.1f}ç§’")
        print(f"åˆ†å—æ•°é‡: {total_chunks} (æ¯å—{chunk_duration}ç§’)")
        
        all_segments = []
        all_text = []
        
        for chunk_idx in range(total_chunks):
            print(f"\nå¤„ç†åˆ†å— {chunk_idx + 1}/{total_chunks}")
            
            # æ£€æŸ¥å†…å­˜
            self.memory_monitor.log_memory_usage(f"åˆ†å—{chunk_idx+1}å¼€å§‹å‰")
            
            # è®¡ç®—å½“å‰å—çš„èŒƒå›´
            start_sample = chunk_idx * chunk_samples
            end_sample = min((chunk_idx + 1) * chunk_samples, total_samples)
            
            # æå–éŸ³é¢‘å—
            chunk_audio = audio[start_sample:end_sample]
            
            # è½¬æ¢ä¸ºWhisperæ ¼å¼
            chunk_audio_whisper = whisper.pad_or_trim(chunk_audio)
            
            # è½¬å½•å½“å‰å—
            try:
                # è¿‡æ»¤æ‰Whisperä¸æ”¯æŒçš„å‚æ•°ï¼Œå¹¶ç¡®ä¿å¯ç”¨æ—¶é—´æˆ³åŠŸèƒ½
                whisper_params = {k: v for k, v in transcribe_kwargs.items() 
                                 if k not in ["segment_duration", "overlap", "use_segmented", "use_memory_optimization"]}
                
                # ç¡®ä¿å¯ç”¨æ—¶é—´æˆ³åŠŸèƒ½
                if "word_timestamps" not in whisper_params:
                    whisper_params["word_timestamps"] = True
                if "no_speech_threshold" not in whisper_params:
                    whisper_params["no_speech_threshold"] = 0.6
                if "logprob_threshold" not in whisper_params:
                    whisper_params["logprob_threshold"] = -1.0
                
                chunk_result = self.model.transcribe(
                    chunk_audio_whisper,
                    **whisper_params
                )
                
                # è°ƒæ•´æ—¶é—´æˆ³
                chunk_start_time = start_sample / sr
                for segment in chunk_result.get("segments", []):
                    segment["start"] += chunk_start_time
                    segment["end"] += chunk_start_time
                    segment["chunk_id"] = chunk_idx
                
                # ä¿å­˜ç»“æœ
                all_segments.extend(chunk_result.get("segments", []))
                all_text.append(chunk_result.get("text", ""))
                
                # æ›´æ–°æ£€æŸ¥ç‚¹
                checkpoint["processed_chunks"].append(chunk_idx)
                checkpoint["current_chunk"] = chunk_idx + 1
                checkpoint["results"].append({
                    "chunk_id": chunk_idx,
                    "text": chunk_result.get("text", ""),
                    "segments": chunk_result.get("segments", []),  # ä¿å­˜å®Œæ•´çš„segmentsæ•°æ®
                    "num_segments": len(chunk_result.get("segments", [])),
                    "start_time": chunk_start_time
                })
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint(checkpoint_path, checkpoint)
                
                print(f"âœ“ åˆ†å— {chunk_idx + 1} å®Œæˆ")
                
                # æ¸…ç†å†…å­˜
                del chunk_audio, chunk_audio_whisper, chunk_result
                gc.collect()
                
            except MemoryError:
                print(f"å†…å­˜ä¸è¶³ï¼å‡å°åˆ†å—å¤§å°å¹¶é‡è¯•...")
                # å‡å°åˆ†å—å¤§å°é‡æ–°å°è¯•
                return self.retry_with_smaller_chunks(audio_path, chunk_duration//2)
            except KeyboardInterrupt:
                print(f"\nè½¬å½•è¢«ç”¨æˆ·ä¸­æ–­")
                self.save_checkpoint(checkpoint_path, checkpoint)
                print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡å¯ä»åˆ†å— {chunk_idx + 1} ç»§ç»­")
                return None
            except Exception as e:
                print(f"åˆ†å— {chunk_idx + 1} å¤„ç†å¤±è´¥: {e}")
                # è·³è¿‡é”™è¯¯çš„åˆ†å—ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                continue
        
        # åˆå¹¶ç»“æœ
        final_result = {
            "text": " ".join(all_text),
            "segments": all_segments,
            "language": transcribe_kwargs.get("language", "unknown"),
            "total_duration": total_samples / sr,
            "num_chunks": total_chunks,
            "processing_time": time.time() - checkpoint["start_time"]
        }
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_result(audio_path, final_result)
        
        # ä¿ç•™æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå³ä½¿æˆåŠŸå®Œæˆè½¬å½•ï¼‰
        if checkpoint_path.exists():
            print(f"âœ“ æ£€æŸ¥ç‚¹æ–‡ä»¶å·²ä¿ç•™: {checkpoint_path}")
        
        print(f"\nå†…å­˜å®‰å…¨è½¬å½•å®Œæˆï¼")
        print(f"æ€»å¤„ç†æ—¶é—´: {final_result['processing_time']:.1f}ç§’")
        print(f"æ€»æ–‡æœ¬é•¿åº¦: {len(final_result['text'])} å­—ç¬¦")
        
        return final_result
    
    def resume_from_checkpoint(self, audio_path: str, checkpoint_path: Path,
                              chunk_duration: int, **transcribe_kwargs) -> Dict:
        """ä»æ£€æŸ¥ç‚¹æ¢å¤è½¬å½•"""
        print("åŠ è½½æ£€æŸ¥ç‚¹...")
        with open(checkpoint_path, 'r', encoding='utf-8') as f:
            checkpoint = json.load(f)
        
        # éªŒè¯éŸ³é¢‘æ–‡ä»¶
        if checkpoint.get("audio_path") != audio_path:
            print("æ£€æŸ¥ç‚¹å¯¹åº”çš„éŸ³é¢‘æ–‡ä»¶ä¸åŒ¹é…ï¼Œå¼€å§‹æ–°çš„è½¬å½•")
            return self.new_transcription(audio_path, checkpoint_path,
                                         chunk_duration, **transcribe_kwargs)
        
        # åˆå¹¶å‚æ•°
        saved_kwargs = checkpoint.get("transcribe_kwargs", {})
        saved_kwargs.update(transcribe_kwargs)
        
        print(f"ä»åˆ†å— {checkpoint['current_chunk']}/{checkpoint['total_chunks']} æ¢å¤")
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=16000)
        total_samples = len(audio)
        chunk_samples = chunk_duration * sr
        
        all_segments = []
        all_text = []
        
        # æ¢å¤å·²å¤„ç†çš„ç»“æœ
        for result in checkpoint.get("results", []):
            all_text.append(result.get("text", ""))
            # æ¢å¤segmentsæ•°æ®ï¼ˆä»æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸­çš„segmentså­—æ®µï¼‰
            if "segments" in result:
                all_segments.extend(result["segments"])
        
        # ç»§ç»­å¤„ç†å‰©ä½™åˆ†å—
        for chunk_idx in range(checkpoint["current_chunk"], checkpoint["total_chunks"]):
            print(f"\nå¤„ç†åˆ†å— {chunk_idx + 1}/{checkpoint['total_chunks']}")
            
            # è®¡ç®—å½“å‰å—çš„èŒƒå›´
            start_sample = chunk_idx * chunk_samples
            end_sample = min((chunk_idx + 1) * chunk_samples, total_samples)
            
            # æå–éŸ³é¢‘å—
            chunk_audio = audio[start_sample:end_sample]
            chunk_audio_whisper = whisper.pad_or_trim(chunk_audio)
            
            # è½¬å½•å½“å‰å—
            try:
                # è¿‡æ»¤æ‰Whisperä¸æ”¯æŒçš„å‚æ•°ï¼Œå¹¶ç¡®ä¿å¯ç”¨æ—¶é—´æˆ³åŠŸèƒ½
                whisper_params = {k: v for k, v in saved_kwargs.items() 
                                 if k not in ["segment_duration", "overlap", "use_segmented", "use_memory_optimization"]}
                
                # ç¡®ä¿å¯ç”¨æ—¶é—´æˆ³åŠŸèƒ½
                if "word_timestamps" not in whisper_params:
                    whisper_params["word_timestamps"] = True
                if "no_speech_threshold" not in whisper_params:
                    whisper_params["no_speech_threshold"] = 0.6
                if "logprob_threshold" not in whisper_params:
                    whisper_params["logprob_threshold"] = -1.0
                
                chunk_result = self.model.transcribe(
                    chunk_audio_whisper,
                    **whisper_params
                )
                
                # è°ƒæ•´æ—¶é—´æˆ³
                chunk_start_time = start_sample / sr
                for segment in chunk_result.get("segments", []):
                    segment["start"] += chunk_start_time
                    segment["end"] += chunk_start_time
                    segment["chunk_id"] = chunk_idx
                
                # ä¿å­˜ç»“æœ
                all_segments.extend(chunk_result.get("segments", []))
                all_text.append(chunk_result.get("text", ""))
                
                # æ›´æ–°æ£€æŸ¥ç‚¹
                checkpoint["processed_chunks"].append(chunk_idx)
                checkpoint["current_chunk"] = chunk_idx + 1
                checkpoint["results"].append({
                    "chunk_id": chunk_idx,
                    "text": chunk_result.get("text", ""),
                    "segments": chunk_result.get("segments", []),  # ä¿å­˜å®Œæ•´çš„segmentsæ•°æ®
                    "num_segments": len(chunk_result.get("segments", [])),
                    "start_time": chunk_start_time
                })
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint(checkpoint_path, checkpoint)
                
                print(f"âœ“ åˆ†å— {chunk_idx + 1} å®Œæˆ")
                
                # æ¸…ç†å†…å­˜
                del chunk_audio, chunk_audio_whisper, chunk_result
                gc.collect()
                
            except KeyboardInterrupt:
                print(f"\nè½¬å½•è¢«ç”¨æˆ·ä¸­æ–­")
                self.save_checkpoint(checkpoint_path, checkpoint)
                print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡å¯ä»åˆ†å— {chunk_idx + 1} ç»§ç»­")
                return None
            except Exception as e:
                print(f"åˆ†å— {chunk_idx + 1} å¤„ç†å¤±è´¥: {e}")
                continue
        
        # åˆå¹¶ç»“æœ
        final_result = {
            "text": " ".join(all_text),
            "segments": all_segments,
            "language": saved_kwargs.get("language", "unknown"),
            "total_duration": total_samples / sr,
            "num_chunks": checkpoint["total_chunks"],
            "processing_time": time.time() - checkpoint["start_time"]
        }
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_result(audio_path, final_result)
        
        # ä¿ç•™æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå³ä½¿æˆåŠŸå®Œæˆè½¬å½•ï¼‰
        if checkpoint_path.exists():
            print(f"âœ“ æ£€æŸ¥ç‚¹æ–‡ä»¶å·²ä¿ç•™: {checkpoint_path}")
        
        print(f"\nè½¬å½•å®Œæˆï¼")
        print(f"æ€»å¤„ç†æ—¶é—´: {final_result['processing_time']:.1f}ç§’")
        
        return final_result
    
    def retry_with_smaller_chunks(self, audio_path: str, new_chunk_duration: int) -> Dict:
        """ä½¿ç”¨æ›´å°çš„åˆ†å—é‡è¯•"""
        print(f"ä½¿ç”¨æ›´å°çš„åˆ†å—: {new_chunk_duration}ç§’")
        self.max_chunk_duration = new_chunk_duration
        
        # åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹
        checkpoint_path = self.get_checkpoint_path(audio_path)
        if checkpoint_path.exists():
            checkpoint_path.unlink()
        
        # é‡æ–°å¼€å§‹
        return self.new_transcription(audio_path, checkpoint_path, new_chunk_duration,
                                     language="ja", fp16=False)
    
    def save_checkpoint(self, checkpoint_path: Path, checkpoint_data: Dict):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_data["last_save"] = time.time()
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    def save_final_result(self, audio_path: str, result: Dict):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        audio_name = Path(audio_path).stem
        
        # ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ç»“æœ
        json_file = Path(audio_path).parent / f"{audio_name}_memory_safe_transcript.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"å†…å­˜å®‰å…¨è½¬å½•ç»“æœå·²ä¿å­˜åˆ°: {json_file}")


class WhisperSegmentResume:
    """
    Whisperåˆ†æ®µè½¬å½•ä¸æ–­ç‚¹ç»­ä¼ 
    å°†é•¿éŸ³é¢‘åˆ†æˆå¤šä¸ªå°æ®µï¼Œé€æ®µè½¬å½•ï¼Œå®ç°çœŸæ­£çš„æ–­ç‚¹ç»­ä¼ 
    """
    
    def __init__(self, model_or_name, device: str = "cpu"):
        try:
            # æ£€æŸ¥ä¼ å…¥çš„æ˜¯æ¨¡å‹å¯¹è±¡è¿˜æ˜¯æ¨¡å‹åç§°
            if isinstance(model_or_name, str):
                # ä¼ å…¥çš„æ˜¯æ¨¡å‹åç§°ï¼Œéœ€è¦åŠ è½½æ¨¡å‹
                print(f"ğŸ“¥ æ­£åœ¨ä¸ºåˆ†æ®µè½¬å½•å™¨åŠ è½½Whisperæ¨¡å‹: {model_or_name}")
                self.model = whisper.load_model(model_or_name, device=device)
            else:
                # ä¼ å…¥çš„æ˜¯å·²åŠ è½½çš„æ¨¡å‹å¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
                print(f"ğŸ“¥ åˆ†æ®µè½¬å½•å™¨é‡ç”¨å·²åŠ è½½çš„æ¨¡å‹")
                self.model = model_or_name
            
            self.device = device
            self.segments_cache = []
            print(f"âœ… åˆ†æ®µè½¬å½•å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            # å°†è¯¦ç»†é”™è¯¯ä¿¡æ¯å†™å…¥æ–‡ä»¶
            error_file = Path("error_log.txt")
            with open(error_file, "w", encoding="utf-8") as f:
                import traceback
                f.write(f"åˆ†æ®µè½¬å½•å™¨åˆå§‹åŒ–å¤±è´¥: {e}\n")
                f.write("å®Œæ•´é”™è¯¯å †æ ˆ:\n")
                f.write(traceback.format_exc())
            print(f"âŒ åˆ†æ®µè½¬å½•å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œè¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ° error_log.txt")
            print(f"âŒ åˆ†æ®µè½¬å½•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def transcribe_long_audio(
        self,
        audio_path: str,
        segment_duration: int = 300,  # æ¯æ®µ5åˆ†é’Ÿ
        overlap: int = 5,  # æ®µé—´é‡å 5ç§’
        checkpoint_dir: str = "whisper_checkpoints",
        **transcribe_kwargs
    ) -> Dict:
        """
        è½¬å½•é•¿éŸ³é¢‘ï¼Œæ”¯æŒçœŸæ­£çš„æ–­ç‚¹ç»­ä¼ 
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            segment_duration: æ¯æ®µæ—¶é•¿ï¼ˆç§’ï¼‰
            overlap: æ®µé—´é‡å æ—¶é•¿ï¼ˆç§’ï¼‰
            checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
            **transcribe_kwargs: Whisperè½¬å½•å‚æ•°
            
        Returns:
            å®Œæ•´çš„è½¬å½•ç»“æœ
        """
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        checkpoint_path = Path(checkpoint_dir)
        checkpoint_path.mkdir(exist_ok=True)
        
        audio_file = Path(audio_path)
        checkpoint_file = checkpoint_path / f"{audio_file.stem}_segments.json"
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=16000)
        total_samples = len(audio)
        segment_samples = segment_duration * sr
        overlap_samples = overlap * sr
        
        # è®¡ç®—æ€»æ®µæ•°
        num_segments = math.ceil(total_samples / segment_samples)
        
        # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
        processed_segments = []
        if checkpoint_file.exists():
            print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_file}")
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                    processed_segments = checkpoint_data.get('segments', [])
                    last_processed = checkpoint_data.get('last_processed', 0)
                    print(f"ğŸ“Š å·²å¤„ç† {len(processed_segments)} æ®µï¼Œä¸Šæ¬¡å¤„ç†åˆ°ç¬¬ {last_processed} æ®µ")
            except Exception as e:
                print(f"âš ï¸ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥ï¼Œä»å¤´å¼€å§‹: {e}")
                last_processed = 0
        else:
            last_processed = 0
        
        # é€æ®µå¤„ç†
        all_segments = []
        
        for seg_idx in range(last_processed, num_segments):
            print(f"\nğŸ¯ å¤„ç†ç¬¬ {seg_idx + 1}/{num_segments} æ®µ...")
            
            # è®¡ç®—å½“å‰æ®µçš„èµ·å§‹å’Œç»“æŸä½ç½®
            start_sample = max(0, seg_idx * segment_samples - (overlap_samples if seg_idx > 0 else 0))
            end_sample = min((seg_idx + 1) * segment_samples + overlap_samples, total_samples)
            
            # æå–éŸ³é¢‘æ®µ
            segment_audio = audio[start_sample:end_sample]
            
            # è½¬å½•å½“å‰æ®µ
            try:
                # è½¬æ¢ä¸ºé€‚åˆWhisperçš„æ ¼å¼
                segment_audio_whisper = whisper.pad_or_trim(segment_audio)
                
                # è½¬å½•
                segment_result = self.model.transcribe(
                    segment_audio_whisper,
                    **transcribe_kwargs
                )
                
                # è°ƒæ•´æ—¶é—´æˆ³
                segment_start_time = start_sample / sr
                for seg in segment_result.get("segments", []):
                    seg["start"] += segment_start_time
                    seg["end"] += segment_start_time
                    seg["segment_id"] = seg_idx
                    seg["segment_start_sample"] = start_sample
                    seg["segment_end_sample"] = end_sample
                
                # æ·»åŠ åˆ°ç»“æœ
                all_segments.extend(segment_result.get("segments", []))
                processed_segments.append({
                    "segment_id": seg_idx,
                    "start_sample": start_sample,
                    "end_sample": end_sample,
                    "start_time": segment_start_time,
                    "num_segments": len(segment_result.get("segments", [])),
                    "text": segment_result.get("text", "")
                })
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                checkpoint_data = {
                    "audio_path": str(audio_path),
                    "total_segments": num_segments,
                    "last_processed": seg_idx + 1,
                    "segments": processed_segments,
                    "timestamp": time.time(),
                    "transcribe_kwargs": transcribe_kwargs
                }
                
                with open(checkpoint_file, 'w', encoding='utf-8') as f:
                    json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… ç¬¬ {seg_idx + 1} æ®µå®Œæˆï¼Œå·²ä¿å­˜æ£€æŸ¥ç‚¹")
                
            except KeyboardInterrupt:
                print("\nâ¸ï¸  è½¬å½•è¢«ç”¨æˆ·ä¸­æ–­")
                print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡å¯ä»ç¬¬ {seg_idx + 1} æ®µç»§ç»­")
                return None
                
            except Exception as e:
                print(f"âŒ ç¬¬ {seg_idx + 1} æ®µå¤„ç†å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ®µ
        
        # åˆå¹¶æ‰€æœ‰æ®µçš„æ–‡æœ¬
        full_text = " ".join([seg.get("text", "") for seg in all_segments])
        
        # æœ€ç»ˆç»“æœ
        final_result = {
            "text": full_text,
            "segments": all_segments,
            "language": transcribe_kwargs.get("language", "unknown"),
            "total_duration": total_samples / sr,
            "num_segments_processed": len(processed_segments),
            "segment_duration": segment_duration,
            "overlap": overlap
        }
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        result_file = audio_file.parent / f"{audio_file.stem}_full_transcript.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        # åˆ é™¤æ£€æŸ¥ç‚¹
        if checkpoint_file.exists():
            checkpoint_file.unlink()
        
        print(f"\nğŸ‰ åˆ†æ®µè½¬å½•å®Œæˆ!")
        print(f"ğŸ“Š æ€»æ—¶é•¿: {total_samples / sr:.2f} ç§’")
        print(f"ğŸ“Š å¤„ç†æ®µæ•°: {len(processed_segments)}")
        print(f"ğŸ“Š æ€»æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
        
        return final_result


class VideoSubtitleGenerator:
    """
    è§†é¢‘å­—å¹•ç”Ÿæˆå™¨ï¼Œæ”¯æŒæœ¬åœ°è§†é¢‘æ–‡ä»¶è¾“å…¥
    åŸºäºWhisperå®ç°éŸ³é¢‘è½¬å½•å’ŒSRTå­—å¹•ç”Ÿæˆï¼Œæ”¯æŒé•¿éŸ³é¢‘åˆ†æ®µè½¬å½•å’Œæ–­ç‚¹ç»­ä¼ 
    æ–°å¢å†…å­˜ä¼˜åŒ–åŠŸèƒ½ï¼Œæ”¯æŒlarge-v2æ¨¡å‹åœ¨16GBå†…å­˜æœºå™¨ä¸Šè¿è¡Œ
    """
    
    def __init__(self, model_name: str = "base", device: str = "cpu", 
                 enable_memory_optimization: bool = False,
                 max_chunk_duration: int = 60):
        """
        åˆå§‹åŒ–å­—å¹•ç”Ÿæˆå™¨
        
        Args:
            model_name: Whisperæ¨¡å‹åç§° (base, small, medium, large, large-v2, large-v3)
            device: è¿è¡Œè®¾å¤‡ (cpu, cuda)
            enable_memory_optimization: æ˜¯å¦å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼
            max_chunk_duration: å†…å­˜ä¼˜åŒ–æ¨¡å¼ä¸‹çš„æœ€å¤§åˆ†å—æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        print(f"ğŸ¯ åˆå§‹åŒ–è§†é¢‘å­—å¹•ç”Ÿæˆå™¨...")
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
        print(f"âš™ï¸  è®¾å¤‡: {device}")
        
        # ç¡®ä¿tempç›®å½•å­˜åœ¨
        self.temp_dir = Path("temp")
        self.temp_dir.mkdir(exist_ok=True)
        
        # åŠ è½½Whisperæ¨¡å‹
        try:
            print(f"ğŸ“¥ æ­£åœ¨åŠ è½½Whisperæ¨¡å‹: {model_name}")
            self.model = whisper.load_model(model_name, device=device)
            self.device = device
            print(f"âœ… Whisperæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–åˆ†æ®µè½¬å½•å™¨ï¼ˆé‡ç”¨å·²åŠ è½½çš„æ¨¡å‹ï¼‰
        try:
            print(f"ğŸ“¥ æ­£åœ¨åˆå§‹åŒ–åˆ†æ®µè½¬å½•å™¨")
            self.segment_transcriber = WhisperSegmentResume(self.model, device)
            print(f"âœ… åˆ†æ®µè½¬å½•å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ åˆ†æ®µè½¬å½•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–è½¬å½•å™¨ï¼ˆä»…åœ¨éœ€è¦æ—¶åŠ è½½ï¼‰
        self.enable_memory_optimization = enable_memory_optimization
        self.memory_efficient_transcriber = None
        if enable_memory_optimization:
            print(f"ğŸ§  å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼")
            print(f"ğŸ“Š æœ€å¤§åˆ†å—æ—¶é•¿: {max_chunk_duration}ç§’")
            try:
                self.memory_efficient_transcriber = MemoryEfficientWhisper(
                    self.model,
                    device=device,
                    max_chunk_duration=max_chunk_duration,
                    checkpoint_dir=str(self.temp_dir / "memory_safe_checkpoints")
                )
                print(f"âœ… å†…å­˜ä¼˜åŒ–è½¬å½•å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ å†…å­˜ä¼˜åŒ–è½¬å½•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                raise
        
        print(f"âœ… æ¨¡å‹å’Œè½¬å½•å™¨åŠ è½½å®Œæˆ")
    
    def extract_audio_from_video(self, video_path: str) -> str:
        """
        ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            æå–çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸµ ä»è§†é¢‘æå–éŸ³é¢‘...")
        
        video_file = Path(video_path)
        if not video_file.exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶å
        audio_filename = f"{video_file.stem}_audio.wav"
        audio_path = self.temp_dir / audio_filename
        
        # ä½¿ç”¨ffmpegæå–éŸ³é¢‘
        try:
            cmd = [
                'ffmpeg', '-y', '-i', str(video_path),
                '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1',
                str(audio_path)
            ]
            
            # ä½¿ç”¨universal_newlines=Falseé¿å…ç¼–ç é—®é¢˜ï¼Œæ‰‹åŠ¨å¤„ç†è¾“å‡º
            result = subprocess.run(cmd, capture_output=True, text=False)
            if result.returncode != 0:
                # å°è¯•ä½¿ç”¨UTF-8ç¼–ç è§£ç é”™è¯¯ä¿¡æ¯
                try:
                    error_msg = result.stderr.decode('utf-8', errors='ignore')
                except:
                    error_msg = result.stderr.decode('gbk', errors='ignore')
                raise RuntimeError(f"éŸ³é¢‘æå–å¤±è´¥: {error_msg}")
            
            print(f"âœ… éŸ³é¢‘æå–å®Œæˆ: {audio_path}")
            return str(audio_path)
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨librosaç›´æ¥è¯»å–è§†é¢‘éŸ³é¢‘
            try:
                print("ğŸ”„ å°è¯•å¤‡ç”¨éŸ³é¢‘æå–æ–¹æ¡ˆ...")
                audio, sr = librosa.load(video_path, sr=16000)
                librosa.output.write_wav(str(audio_path), audio, sr)
                print(f"âœ… å¤‡ç”¨æ–¹æ¡ˆéŸ³é¢‘æå–å®Œæˆ: {audio_path}")
                return str(audio_path)
            except Exception as fallback_e:
                raise RuntimeError(f"æ‰€æœ‰éŸ³é¢‘æå–æ–¹æ³•å‡å¤±è´¥: {fallback_e}")
    
    def transcribe_audio(self, audio_path: str, **transcribe_kwargs) -> Dict:
        """
        è½¬å½•éŸ³é¢‘æ–‡ä»¶ï¼Œæ”¯æŒåˆ†æ®µè½¬å½•ã€æ–­ç‚¹ç»­ä¼ å’Œå†…å­˜ä¼˜åŒ–
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            **transcribe_kwargs: Whisperè½¬å½•å‚æ•°
            
        Returns:
            è½¬å½•ç»“æœå­—å…¸
        """
        print(f"ğŸ¤ å¼€å§‹éŸ³é¢‘è½¬å½•...")
        
        # é»˜è®¤è½¬å½•å‚æ•°
        default_params = {
            "language": "ja",  # é»˜è®¤æ—¥è¯­è½¬å½•
            "task": "transcribe",
            "fp16": False,
            "verbose": False,
            "temperature": 0.0,
            "best_of": 1,
            "beam_size": 1,
            "word_timestamps": True,
            "suppress_tokens": [-1],
            "initial_prompt": None
        }
        
        # åˆå¹¶å‚æ•°
        params = {**default_params, **transcribe_kwargs}
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å†…å­˜ä¼˜åŒ–è½¬å½•
        use_memory_optimization = transcribe_kwargs.get("use_memory_optimization", False)
        
        # å¦‚æœå¯ç”¨äº†å†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼Œä¼˜å…ˆä½¿ç”¨å†…å­˜ä¼˜åŒ–è½¬å½•
        if self.enable_memory_optimization and use_memory_optimization:
            print(f"ğŸ§  å¯ç”¨å†…å­˜ä¼˜åŒ–è½¬å½•æ¨¡å¼")
            
            try:
                # ä½¿ç”¨å†…å­˜ä¼˜åŒ–è½¬å½•å™¨
                result = self.memory_efficient_transcriber.transcribe_with_memory_safety(
                    audio_path, **params
                )
                
                if result is None:
                    raise KeyboardInterrupt("è½¬å½•è¢«ç”¨æˆ·ä¸­æ–­")
                
                # æ·»åŠ å…ƒæ•°æ®
                result["audio_path"] = audio_path
                result["transcription_time"] = time.time()
                result["transcription_params"] = params
                result["transcription_mode"] = "memory_optimized"
                
                print(f"âœ… å†…å­˜ä¼˜åŒ–éŸ³é¢‘è½¬å½•å®Œæˆ")
                print(f"ğŸ“Š è¯†åˆ«ç‰‡æ®µæ•°: {len(result.get('segments', []))}")
                print(f"ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: {len(result.get('text', ''))} å­—ç¬¦")
                
                return result
                
            except Exception as e:
                print(f"âš ï¸ å†…å­˜ä¼˜åŒ–è½¬å½•å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ¨¡å¼: {e}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µè½¬å½•
        segment_duration = transcribe_kwargs.get("segment_duration", 0)
        use_segmented = transcribe_kwargs.get("use_segmented", False)
        
        # è·å–éŸ³é¢‘æ—¶é•¿
        try:
            audio_info = librosa.load(audio_path, sr=16000)
            audio_duration = len(audio_info[0]) / audio_info[1]
            
            # å¦‚æœéŸ³é¢‘æ—¶é•¿è¶…è¿‡10åˆ†é’Ÿæˆ–æ˜ç¡®æŒ‡å®šä½¿ç”¨åˆ†æ®µè½¬å½•ï¼Œåˆ™å¯ç”¨åˆ†æ®µæ¨¡å¼
            if audio_duration > 600 or use_segmented:  # 10åˆ†é’Ÿ
                print(f"ğŸ“Š éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f} ç§’ ({timedelta(seconds=int(audio_duration))})")
                print(f"ğŸ”€ å¯ç”¨åˆ†æ®µè½¬å½•æ¨¡å¼")
                
                # è®¾ç½®åˆ†æ®µå‚æ•°
                segment_duration = segment_duration if segment_duration > 0 else 300  # é»˜è®¤5åˆ†é’Ÿä¸€æ®µ
                overlap = transcribe_kwargs.get("overlap", 5)  # é»˜è®¤é‡å 5ç§’
                checkpoint_dir = str(self.temp_dir / "whisper_checkpoints")
                
                print(f"âš™ï¸  åˆ†æ®µå‚æ•°: æ¯æ®µ {segment_duration} ç§’ï¼Œé‡å  {overlap} ç§’")
                
                # è¿‡æ»¤æ‰åˆ†æ®µè½¬å½•ç›¸å…³çš„å‚æ•°ï¼Œåªä¿ç•™Whisperè½¬å½•å‚æ•°
                whisper_params = {k: v for k, v in params.items() 
                                 if k not in ["segment_duration", "overlap", "use_segmented", "use_memory_optimization"]}
                
                # ä½¿ç”¨åˆ†æ®µè½¬å½•
                result = self.segment_transcriber.transcribe_long_audio(
                    audio_path=audio_path,
                    segment_duration=segment_duration,
                    overlap=overlap,
                    checkpoint_dir=checkpoint_dir,
                    **whisper_params
                )
                
                if result is None:
                    raise KeyboardInterrupt("è½¬å½•è¢«ç”¨æˆ·ä¸­æ–­")
                
                # æ·»åŠ å…ƒæ•°æ®
                result["audio_path"] = audio_path
                result["transcription_time"] = time.time()
                result["transcription_params"] = params
                result["transcription_mode"] = "segmented"
                
                print(f"âœ… åˆ†æ®µéŸ³é¢‘è½¬å½•å®Œæˆ")
                print(f"ğŸ“Š è¯†åˆ«ç‰‡æ®µæ•°: {len(result.get('segments', []))}")
                print(f"ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: {len(result.get('text', ''))} å­—ç¬¦")
                
                return result
            
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘æ—¶é•¿æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†è½¬å½•æ¨¡å¼: {e}")
        
        # æ ‡å‡†è½¬å½•æ¨¡å¼
        try:
            print(f"ğŸ”€ ä½¿ç”¨æ ‡å‡†è½¬å½•æ¨¡å¼")
            # è¿‡æ»¤æ‰éWhisperå‚æ•°
            whisper_params = {k: v for k, v in params.items() 
                             if k not in ["segment_duration", "overlap", "use_segmented", "use_memory_optimization"]}
            result = self.model.transcribe(audio_path, **whisper_params)
            
            # æ·»åŠ å…ƒæ•°æ®
            result["audio_path"] = audio_path
            result["transcription_time"] = time.time()
            result["transcription_params"] = params
            result["transcription_mode"] = "standard"
            
            print(f"âœ… éŸ³é¢‘è½¬å½•å®Œæˆ")
            print(f"ğŸ“Š è¯†åˆ«ç‰‡æ®µæ•°: {len(result.get('segments', []))}")
            print(f"ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: {len(result.get('text', ''))} å­—ç¬¦")
            
            return result
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘è½¬å½•å¤±è´¥: {e}")
            raise
    
    def generate_srt_content(self, transcription_result: Dict) -> str:
        """
        ç”ŸæˆSRTæ ¼å¼çš„å­—å¹•å†…å®¹
        
        Args:
            transcription_result: è½¬å½•ç»“æœ
            
        Returns:
            SRTæ ¼å¼çš„å­—å¹•å†…å®¹
        """
        print(f"ğŸ“ ç”ŸæˆSRTå­—å¹•å†…å®¹...")
        
        segments = transcription_result.get("segments", [])
        text = transcription_result.get("text", "").strip()
        
        if not segments and not text:
            raise ValueError("è½¬å½•ç»“æœä¸­æ²¡æœ‰æœ‰æ•ˆçš„ç‰‡æ®µæˆ–æ–‡æœ¬å†…å®¹")
        
        srt_content = ""
        
        if segments:
            # å¦‚æœæœ‰æ—¶é—´æˆ³ç‰‡æ®µï¼Œä½¿ç”¨ç‰‡æ®µä¿¡æ¯
            for i, segment in enumerate(segments):
                # æ ¼å¼åŒ–æ—¶é—´æˆ³ (SRTæ ¼å¼: HH:MM:SS,mmm)
                start_time = self.format_time_srt(segment["start"])
                end_time = self.format_time_srt(segment["end"])
                
                # è·å–æ–‡æœ¬å†…å®¹
                text = segment.get("text", "").strip()
                
                # æ„å»ºSRTæ¡ç›®
                srt_content += f"{i+1}\n"
                srt_content += f"{start_time} --> {end_time}\n"
                srt_content += f"{text}\n\n"
            
            print(f"âœ… SRTå†…å®¹ç”Ÿæˆå®Œæˆï¼Œå…± {len(segments)} ä¸ªå­—å¹•æ¡ç›®")
        else:
            # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ç‰‡æ®µä½†æœ‰æ–‡æœ¬å†…å®¹ï¼Œç”Ÿæˆè™šæ‹Ÿæ—¶é—´æˆ³
            print(f"âš ï¸  è½¬å½•ç»“æœä¸­æ²¡æœ‰æ—¶é—´æˆ³ç‰‡æ®µï¼Œç”Ÿæˆè™šæ‹Ÿæ—¶é—´æˆ³")
            
            # åˆ†å‰²æ–‡æœ¬ä¸ºæ®µè½
            paragraphs = [p.strip() for p in text.split('ã€‚') if p.strip()]
            
            # è®¡ç®—æ€»æ—¶é•¿
            total_duration = transcription_result.get("total_duration", 300)  # é»˜è®¤5åˆ†é’Ÿ
            
            # ä¸ºæ¯ä¸ªæ®µè½åˆ†é…æ—¶é—´
            for i, paragraph in enumerate(paragraphs):
                # è®¡ç®—æ¯ä¸ªæ®µè½çš„æŒç»­æ—¶é—´ï¼ˆå¹³å‡åˆ†é…ï¼‰
                segment_duration = total_duration / max(len(paragraphs), 1)
                start_time = i * segment_duration
                end_time = min((i + 1) * segment_duration, total_duration)
                
                # æ ¼å¼åŒ–æ—¶é—´æˆ³
                start_time_str = self.format_time_srt(start_time)
                end_time_str = self.format_time_srt(end_time)
                
                # æ„å»ºSRTæ¡ç›®
                srt_content += f"{i+1}\n"
                srt_content += f"{start_time_str} --> {end_time_str}\n"
                srt_content += f"{paragraph}ã€‚\n\n"
            
            print(f"âœ… SRTå†…å®¹ç”Ÿæˆå®Œæˆï¼Œå…± {len(paragraphs)} ä¸ªè™šæ‹Ÿå­—å¹•æ¡ç›®")
        
        return srt_content
    
    def format_time_srt(self, seconds: float) -> str:
        """
        å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºSRTæ—¶é—´æ ¼å¼ (HH:MM:SS,mmm)
        
        Args:
            seconds: ç§’æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds_remainder = seconds % 60
        milliseconds = int((seconds_remainder - int(seconds_remainder)) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{int(seconds_remainder):02d},{milliseconds:03d}"
    
    def save_srt_file(self, srt_content: str, video_path: str, output_dir: str = None) -> str:
        """
        ä¿å­˜SRTæ–‡ä»¶
        
        Args:
            srt_content: SRTå†…å®¹
            video_path: åŸå§‹è§†é¢‘è·¯å¾„ï¼ˆç”¨äºç”Ÿæˆæ–‡ä»¶åï¼‰
            output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨tempç›®å½•ï¼‰
            
        Returns:
            ä¿å­˜çš„SRTæ–‡ä»¶è·¯å¾„
        """
        if output_dir is None:
            output_dir = self.temp_dir
        else:
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True)
        
        video_file = Path(video_path)
        srt_filename = f"{video_file.stem}.srt"
        srt_path = output_dir / srt_filename
        
        # å†™å…¥æ–‡ä»¶
        with open(srt_path, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        
        print(f"ğŸ’¾ SRTæ–‡ä»¶å·²ä¿å­˜: {srt_path}")
        return str(srt_path)
    
    def save_transcription_result(self, result: Dict, video_path: str) -> str:
        """
        ä¿å­˜è½¬å½•ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            result: è½¬å½•ç»“æœ
            video_path: åŸå§‹è§†é¢‘è·¯å¾„
            
        Returns:
            JSONæ–‡ä»¶è·¯å¾„
        """
        video_file = Path(video_path)
        json_filename = f"{video_file.stem}_transcription.json"
        json_path = self.temp_dir / json_filename
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è½¬å½•ç»“æœå·²ä¿å­˜: {json_path}")
        return str(json_path)
    
    def cleanup_temp_files(self, keep_audio: bool = False, keep_json: bool = False, keep_srt: bool = False):
        """
        æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        
        Args:
            keep_audio: æ˜¯å¦ä¿ç•™éŸ³é¢‘æ–‡ä»¶
            keep_json: æ˜¯å¦ä¿ç•™JSONè½¬å½•ç»“æœ
            keep_srt: æ˜¯å¦ä¿ç•™SRTå­—å¹•æ–‡ä»¶
        """
        print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        
        temp_files = list(self.temp_dir.glob("*"))
        
        for file_path in temp_files:
            if file_path.is_file():
                if keep_audio and "_audio.wav" in file_path.name:
                    continue
                if keep_json and "_transcription.json" in file_path.name:
                    continue
                if keep_srt and file_path.suffix == ".srt":
                    continue
                
                try:
                    file_path.unlink()
                    print(f"ğŸ—‘ï¸  å·²åˆ é™¤: {file_path.name}")
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤å¤±è´¥ {file_path.name}: {e}")
            elif file_path.is_dir():
                # æ¸…ç†æ£€æŸ¥ç‚¹ç›®å½•
                if file_path.name in ["memory_safe_checkpoints", "whisper_checkpoints"]:
                    try:
                        # åˆ é™¤ç›®å½•åŠå…¶æ‰€æœ‰å†…å®¹
                        import shutil
                        shutil.rmtree(file_path)
                        print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ£€æŸ¥ç‚¹ç›®å½•: {file_path.name}")
                    except Exception as e:
                        print(f"âš ï¸  åˆ é™¤æ£€æŸ¥ç‚¹ç›®å½•å¤±è´¥ {file_path.name}: {e}")
        
        print(f"âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
    
    def generate_subtitles(self, video_path: str, 
                          output_srt: bool = True,
                          output_json: bool = True,
                          cleanup: bool = True,
                          segment_duration: int = 0,
                          overlap: int = 5,
                          use_segmented: bool = False,
                          use_memory_optimization: bool = False,
                          **transcribe_kwargs) -> Dict:
        """
        ç”Ÿæˆè§†é¢‘å­—å¹•çš„å®Œæ•´æµç¨‹ï¼Œæ”¯æŒåˆ†æ®µè½¬å½•å’Œå†…å­˜ä¼˜åŒ–
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_srt: æ˜¯å¦è¾“å‡ºSRTæ–‡ä»¶
            output_json: æ˜¯å¦è¾“å‡ºJSONè½¬å½•ç»“æœ
            cleanup: æ˜¯å¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            segment_duration: åˆ†æ®µæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºè‡ªåŠ¨åˆ¤æ–­
            overlap: æ®µé—´é‡å æ—¶é•¿ï¼ˆç§’ï¼‰
            use_segmented: å¼ºåˆ¶ä½¿ç”¨åˆ†æ®µè½¬å½•
            use_memory_optimization: å¼ºåˆ¶ä½¿ç”¨å†…å­˜ä¼˜åŒ–è½¬å½•
            **transcribe_kwargs: Whisperè½¬å½•å‚æ•°
            
        Returns:
            åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
        """
        print("=" * 60)
        print(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {Path(video_path).name}")
        print("=" * 60)
        
        start_time = time.time()
        result = {
            "video_path": video_path,
            "processing_start_time": start_time,
            "steps": {}
        }
        
        try:
            # æ­¥éª¤1: æå–éŸ³é¢‘
            audio_start = time.time()
            audio_path = self.extract_audio_from_video(video_path)
            result["audio_path"] = audio_path
            result["steps"]["audio_extraction"] = time.time() - audio_start
            
            # æ­¥éª¤2: è½¬å½•éŸ³é¢‘
            transcribe_start = time.time()
            
            # å¦‚æœå¯ç”¨äº†å†…å­˜ä¼˜åŒ–åŠŸèƒ½ï¼Œè‡ªåŠ¨å¯ç”¨å†…å­˜ä¼˜åŒ–è½¬å½•æ¨¡å¼
            if self.enable_memory_optimization and not use_memory_optimization:
                use_memory_optimization = True
                print(f"ğŸ§  æ£€æµ‹åˆ°å†…å­˜ä¼˜åŒ–åŠŸèƒ½å·²å¯ç”¨ï¼Œè‡ªåŠ¨å¯ç”¨å†…å­˜ä¼˜åŒ–è½¬å½•æ¨¡å¼")
            
            # æ·»åŠ åˆ†æ®µè½¬å½•å’Œå†…å­˜ä¼˜åŒ–å‚æ•°
            transcribe_params = {
                **transcribe_kwargs,
                "segment_duration": segment_duration,
                "overlap": overlap,
                "use_segmented": use_segmented,
                "use_memory_optimization": use_memory_optimization
            }
            
            transcription_result = self.transcribe_audio(audio_path, **transcribe_params)
            result["transcription_result"] = transcription_result
            result["steps"]["audio_transcription"] = time.time() - transcribe_start
            
            # è®°å½•è½¬å½•æ¨¡å¼
            result["transcription_mode"] = transcription_result.get("transcription_mode", "unknown")
            
            # æ­¥éª¤3: ç”ŸæˆSRTå†…å®¹
            srt_start = time.time()
            srt_content = self.generate_srt_content(transcription_result)
            result["srt_content"] = srt_content
            result["steps"]["srt_generation"] = time.time() - srt_start
            
            # æ­¥éª¤4: ä¿å­˜æ–‡ä»¶
            save_start = time.time()
            
            if output_srt:
                srt_path = self.save_srt_file(srt_content, video_path)
                result["srt_path"] = srt_path
            
            if output_json:
                json_path = self.save_transcription_result(transcription_result, video_path)
                result["json_path"] = json_path
            
            result["steps"]["file_saving"] = time.time() - save_start
            
            # æ­¥éª¤5: æ¸…ç†
            if cleanup:
                cleanup_start = time.time()
                self.cleanup_temp_files(keep_audio=not cleanup, keep_json=output_json, keep_srt=output_srt)
                result["steps"]["cleanup"] = time.time() - cleanup_start
            
            # è®¡ç®—æ€»æ—¶é—´
            total_time = time.time() - start_time
            result["processing_end_time"] = time.time()
            result["total_processing_time"] = total_time
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            print("=" * 60)
            print(f"âœ… å¤„ç†å®Œæˆ!")
            print("=" * 60)
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   è§†é¢‘æ–‡ä»¶: {Path(video_path).name}")
            print(f"   è½¬å½•æ¨¡å¼: {result.get('transcription_mode', 'unknown')}")
            print(f"   æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
            print(f"   è¯†åˆ«ç‰‡æ®µæ•°: {len(transcription_result.get('segments', []))}")
            print(f"   æ€»æ–‡æœ¬é•¿åº¦: {len(transcription_result.get('text', ''))} å­—ç¬¦")
            
            # å¦‚æœæ˜¯åˆ†æ®µè½¬å½•ï¼Œæ˜¾ç¤ºåˆ†æ®µä¿¡æ¯
            if result.get('transcription_mode') == 'segmented':
                print(f"   åˆ†æ®µå‚æ•°: {segment_duration}ç§’/æ®µï¼Œé‡å {overlap}ç§’")
            
            # å¦‚æœæ˜¯å†…å­˜ä¼˜åŒ–è½¬å½•ï¼Œæ˜¾ç¤ºå†…å­˜ä¼˜åŒ–ä¿¡æ¯
            if result.get('transcription_mode') == 'memory_optimized':
                # ä»è½¬å½•ç»“æœä¸­è·å–å®é™…ä½¿ç”¨çš„åˆ†å—æ—¶é•¿
                actual_chunk_duration = transcription_result.get('num_chunks', 0)
                if actual_chunk_duration > 0:
                    total_duration = transcription_result.get('total_duration', 0)
                    avg_chunk_duration = total_duration / actual_chunk_duration
                    print(f"   å†…å­˜ä¼˜åŒ–: å·²å¯ç”¨ï¼Œå¹³å‡åˆ†å—æ—¶é•¿{avg_chunk_duration:.1f}ç§’")
                else:
                    print(f"   å†…å­˜ä¼˜åŒ–: å·²å¯ç”¨")
            
            if output_srt:
                print(f"   SRTæ–‡ä»¶: {result.get('srt_path', 'æœªç”Ÿæˆ')}")
            if output_json:
                print(f"   JSONæ–‡ä»¶: {result.get('json_path', 'æœªç”Ÿæˆ')}")
            
            print(f"â±ï¸  å„æ­¥éª¤è€—æ—¶:")
            for step, duration in result["steps"].items():
                print(f"     {step}: {duration:.2f}ç§’")
            
            return result
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            result["error"] = str(e)
            result["processing_end_time"] = time.time()
            result["total_processing_time"] = time.time() - start_time
            
            # å‘ç”Ÿé”™è¯¯æ—¶ä¿ç•™ä¸´æ—¶æ–‡ä»¶ä»¥ä¾¿è°ƒè¯•
            print(f"âš ï¸  å‘ç”Ÿé”™è¯¯ï¼Œä¸´æ—¶æ–‡ä»¶å°†ä¿ç•™åœ¨ {self.temp_dir}")
            
            return result


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è§†é¢‘å­—å¹•ç”Ÿæˆå·¥å…·")
    parser.add_argument("video_path", help="è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", default="base", 
                       choices=["tiny", "base", "small", "medium", "large", "large-v2", "large-v3"],
                       help="Whisperæ¨¡å‹å¤§å° (é»˜è®¤: base)")
    parser.add_argument("--device", default="cpu", choices=["cpu", "cuda"],
                       help="è¿è¡Œè®¾å¤‡ (é»˜è®¤: cpu)")
    parser.add_argument("--language", default="ja", 
                       help="è½¬å½•è¯­è¨€ä»£ç  (é»˜è®¤: ja - æ—¥è¯­)")
    parser.add_argument("--output-dir", help="è¾“å‡ºç›®å½• (é»˜è®¤: temp)")
    parser.add_argument("--no-srt", action="store_true", help="ä¸ç”ŸæˆSRTæ–‡ä»¶")
    parser.add_argument("--no-json", action="store_true", help="ä¸ç”ŸæˆJSONæ–‡ä»¶")
    parser.add_argument("--clean", action="store_true", help="æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
    parser.add_argument("--keep-audio", action="store_true", help="ä¿ç•™éŸ³é¢‘æ–‡ä»¶")
    
    # åˆ†æ®µè½¬å½•å‚æ•°
    parser.add_argument("--segment-duration", type=int, default=0,
                       help="åˆ†æ®µæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºè‡ªåŠ¨åˆ¤æ–­ (é»˜è®¤: 0)")
    parser.add_argument("--overlap", type=int, default=5,
                       help="æ®µé—´é‡å æ—¶é•¿ï¼ˆç§’ï¼‰ (é»˜è®¤: 5)")
    parser.add_argument("--force-segmented", action="store_true",
                       help="å¼ºåˆ¶ä½¿ç”¨åˆ†æ®µè½¬å½•æ¨¡å¼")
    
    # å†…å­˜ä¼˜åŒ–å‚æ•°
    parser.add_argument("--enable-memory-optimization", action="store_true",
                       help="å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼Œæ”¯æŒlarge-v2æ¨¡å‹åœ¨16GBå†…å­˜æœºå™¨ä¸Šè¿è¡Œ")
    parser.add_argument("--max-chunk-duration", type=int, default=60,
                       help="å†…å­˜ä¼˜åŒ–æ¨¡å¼ä¸‹çš„æœ€å¤§åˆ†å—æ—¶é•¿ï¼ˆç§’ï¼‰ (é»˜è®¤: 60)")
    parser.add_argument("--force-memory-optimized", action="store_true",
                       help="å¼ºåˆ¶ä½¿ç”¨å†…å­˜ä¼˜åŒ–è½¬å½•æ¨¡å¼")
    
    args = parser.parse_args()
    
    # éªŒè¯è§†é¢‘æ–‡ä»¶å­˜åœ¨
    if not Path(args.video_path).exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.video_path}")
        return 1
    
    # åˆå§‹åŒ–å­—å¹•ç”Ÿæˆå™¨
    try:
        generator = VideoSubtitleGenerator(
            model_name=args.model,
            device=args.device,
            enable_memory_optimization=args.enable_memory_optimization,
            max_chunk_duration=args.max_chunk_duration
        )
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        generator.temp_dir = Path(args.output_dir)
        generator.temp_dir.mkdir(exist_ok=True)
    
    # è½¬å½•å‚æ•°
    transcribe_params = {
        "language": args.language
    }
    
    # ç”Ÿæˆå­—å¹•
    result = generator.generate_subtitles(
        video_path=args.video_path,
        output_srt=not args.no_srt,
        output_json=not args.no_json,
        cleanup=args.clean,  # åªæœ‰å½“ç”¨æˆ·æŒ‡å®š--cleanæ—¶æ‰æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        segment_duration=args.segment_duration,
        overlap=args.overlap,
        use_segmented=args.force_segmented,
        use_memory_optimization=args.force_memory_optimized,
        **transcribe_params
    )
    
    # å¤„ç†ç»“æœ
    if "error" in result:
        print(f"âŒ å­—å¹•ç”Ÿæˆå¤±è´¥: {result['error']}")
        return 1
    else:
        print(f"ğŸ‰ å­—å¹•ç”ŸæˆæˆåŠŸ!")
        return 0


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    if len(os.sys.argv) == 1:
        print("ğŸ¯ è§†é¢‘å­—å¹•ç”Ÿæˆå·¥å…·")
        print("=" * 50)
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python whisper-translation.py <è§†é¢‘æ–‡ä»¶è·¯å¾„> [é€‰é¡¹]")
        print("")
        print("é€‰é¡¹:")
        print("  --model MODEL        Whisperæ¨¡å‹ (tiny, base, small, medium, large, large-v2, large-v3)")
        print("  --device DEVICE      è¿è¡Œè®¾å¤‡ (cpu, cuda)")
        print("  --language LANG      è½¬å½•è¯­è¨€ä»£ç  (ja, zh, enç­‰)")
        print("  --output-dir DIR     è¾“å‡ºç›®å½•")
        print("  --no-srt             ä¸ç”ŸæˆSRTæ–‡ä»¶")
        print("  --no-json            ä¸ç”ŸæˆJSONæ–‡ä»¶")
        print("  --clean              æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆé»˜è®¤ä¿ç•™ï¼‰")
        print("  --keep-audio         ä¿ç•™éŸ³é¢‘æ–‡ä»¶")
        print("")
        print("åˆ†æ®µè½¬å½•é€‰é¡¹ (ç”¨äºå¤„ç†é•¿è§†é¢‘):")
        print("  --segment-duration SEC  åˆ†æ®µæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºè‡ªåŠ¨åˆ¤æ–­")
        print("  --overlap SEC           æ®µé—´é‡å æ—¶é•¿ï¼ˆç§’ï¼‰")
        print("  --force-segmented       å¼ºåˆ¶ä½¿ç”¨åˆ†æ®µè½¬å½•æ¨¡å¼")
        print("")
        print("å†…å­˜ä¼˜åŒ–é€‰é¡¹ (ç”¨äºæœ‰é™å†…å­˜ç¯å¢ƒ):")
        print("  --enable-memory-optimization  å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼")
        print("  --max-chunk-duration SEC      æœ€å¤§åˆ†å—æ—¶é•¿ï¼ˆç§’ï¼‰")
        print("  --force-memory-optimized      å¼ºåˆ¶ä½¿ç”¨å†…å­˜ä¼˜åŒ–è½¬å½•")
        print("")
        print("ç¤ºä¾‹:")
        print("  python whisper-translation.py my_video.mp4 --model base --language ja")
        print("  python whisper-translation.py video.avi --model large-v3 --device cuda")
        print("  python whisper-translation.py long_movie.mp4 --segment-duration 300 --overlap 10")
        print("  python whisper-translation.py lecture.mp4 --force-segmented --segment-duration 600")
        print("  python whisper-translation.py big_video.mp4 --model large-v2 --enable-memory-optimization")
        print("  python whisper-translation.py hd_video.mp4 --model large-v3 --force-memory-optimized --max-chunk-duration 30")
        print("")
        
        # æµ‹è¯•ç¤ºä¾‹
        test_video = input("è¾“å…¥æµ‹è¯•è§†é¢‘è·¯å¾„ (æˆ–æŒ‰å›è½¦è·³è¿‡): ").strip()
        if test_video and Path(test_video).exists():
            print(f"\nğŸ¬ å¼€å§‹æµ‹è¯•å¤„ç†: {test_video}")
            
            generator = VideoSubtitleGenerator(model_name="base", device="cpu")
            result = generator.generate_subtitles(
                video_path=test_video,
                output_srt=True,
                output_json=True,
                cleanup=False,  # æµ‹è¯•æ—¶ä¿ç•™æ–‡ä»¶ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
                language="ja"
            )
        else:
            print("âŒ æœªæä¾›æœ‰æ•ˆçš„æµ‹è¯•è§†é¢‘è·¯å¾„")
    else:
        # æ­£å¸¸å‘½ä»¤è¡Œæ‰§è¡Œ
        exit(main())
