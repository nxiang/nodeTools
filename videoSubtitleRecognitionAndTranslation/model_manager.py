"""
æ¨¡åž‹ç®¡ç†æ¨¡å—
è´Ÿè´£Whisperæ¨¡åž‹çš„åŠ è½½ã€ç¼“å­˜å’Œé…ç½®
"""

import os
import whisper

def check_cpu_availability():
    """æ£€æŸ¥CPUä¿¡æ¯"""
    try:
        import psutil
        cpu_count = psutil.cpu_count(logical=False)  # ç‰©ç†æ ¸å¿ƒæ•°
        logical_cpu_count = psutil.cpu_count(logical=True)  # é€»è¾‘æ ¸å¿ƒæ•°
        memory = psutil.virtual_memory()
        memory_gb = memory.total / 1024**3
        
        return f"CPU: {cpu_count}æ ¸/{logical_cpu_count}çº¿ç¨‹, å†…å­˜: {memory_gb:.1f}GB"
    except ImportError:
        return "CPUæ¨¡å¼ï¼ˆpsutilæœªå®‰è£…ï¼Œæ— æ³•èŽ·å–è¯¦ç»†ä¿¡æ¯ï¼‰"

def setup_whisper_model(model_size='medium'):
    """è®¾ç½®Whisperæ¨¡åž‹ï¼ˆCPUæ¨¡å¼ï¼‰"""
    print("ðŸ’» ä½¿ç”¨CPUå¤„ç†æ¨¡å¼")
    
    # è®¾ç½®ç¼“å­˜è·¯å¾„ï¼Œé¿å…é‡å¤ä¸‹è½½
    cache_dir = os.path.expanduser('~/.cache/whisper')
    os.environ['WHISPER_CACHE_DIR'] = cache_dir
    
    print(f"ðŸ“¥ åŠ è½½Whisper {model_size}æ¨¡åž‹...")
    
    # é¢„æœŸçš„æ¨¡åž‹æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰- æ›´æ–°ä¸ºå®žé™…å¤§å°
    expected_sizes = {
        'tiny': 75_572_083,
        'base': 142_000_000,
        'small': 466_000_000,
        'medium': 1_528_008_539,
        'large': 3_087_371_615,  # ä¿®æ­£ä¸ºå®žé™…æ–‡ä»¶å¤§å°ï¼Œå¹¶ç»Ÿä¸€ä½¿ç”¨'large'ä½œä¸ºå‚æ•°
    }
    
    # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
    model_file = os.path.join(cache_dir, f'{model_size}.pt')
    
    # å¯¹äºŽlargeæ¨¡åž‹ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨large-v3.ptæ–‡ä»¶
    if model_size == 'large' and not os.path.exists(model_file):
        large_v3_file = os.path.join(cache_dir, 'large-v3.pt')
        if os.path.exists(large_v3_file):
            print(f"ðŸ”„ å‘çŽ°large-v3.ptæ–‡ä»¶ï¼Œåˆ›å»ºç¬¦å·é“¾æŽ¥ä¸ºlarge.pt")
            try:
                # åˆ›å»ºç¬¦å·é“¾æŽ¥æˆ–å¤åˆ¶æ–‡ä»¶
                if os.name == 'nt':  # Windowsç³»ç»Ÿ
                    import shutil
                    shutil.copy2(large_v3_file, model_file)
                else:  # Unixç³»ç»Ÿ
                    os.symlink(large_v3_file, model_file)
                print(f"âœ… å·²åˆ›å»ºlarge.ptæ–‡ä»¶")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åˆ›å»ºlarge.ptæ–‡ä»¶: {e}")
    
    if os.path.exists(model_file):
        # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
        file_size = os.path.getsize(model_file)
        expected_size = expected_sizes.get(model_size, 0)
        
        if expected_size > 0 and file_size < expected_size * 0.9:
            print(f"âš ï¸ æ¨¡åž‹æ–‡ä»¶å¯èƒ½æŸå: {file_size:,} bytes < é¢„æœŸ {expected_size:,} bytes")
            print("ðŸ—‘ï¸ åˆ é™¤æŸåæ–‡ä»¶å¹¶é‡æ–°ä¸‹è½½...")
            try:
                os.remove(model_file)
            except Exception as e:
                print(f"âŒ åˆ é™¤å¤±è´¥: {e}")
        else:
            print(f"âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ¨¡åž‹: {model_file}")
            print(f"ðŸ“Š æ–‡ä»¶å¤§å°: {file_size:,} bytes")
    else:
        print(f"ðŸ“¡ ä¸‹è½½æ¨¡åž‹åˆ°ç¼“å­˜ç›®å½•: {cache_dir}")
    
    # çº¿ç¨‹å®‰å…¨çš„æ¨¡åž‹åŠ è½½
    try:
        # è®¾ç½®çº¿ç¨‹å¼‚å¸¸å¤„ç†
        import threading
        threading.excepthook = lambda args: print(f"âš ï¸ çº¿ç¨‹å¼‚å¸¸: {args.exc_type.__name__}: {args.exc_value}")
        
        # æŠ‘åˆ¶FP16è­¦å‘Š
        import warnings
        warnings.filterwarnings("ignore", message="FP16 is not supported on CPU; using FP32 instead")
        
        # åŠ è½½æ¨¡åž‹
        model = whisper.load_model(model_size, device="cpu")
        print("âœ… æ¨¡åž‹åŠ è½½æˆåŠŸ")
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
        # å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡åž‹ä½œä¸ºå¤‡é€‰
        if model_size != 'tiny':
            print(f"ðŸ”„ å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡åž‹ä½œä¸ºå¤‡é€‰...")
            # æŒ‰å¤§å°é¡ºåºå°è¯•å¤‡é€‰æ¨¡åž‹
            model_priority = ['medium', 'small', 'base', 'tiny']
            current_index = model_priority.index(model_size) if model_size in model_priority else 0
            
            for next_model in model_priority[current_index + 1:]:
                print(f"  å°è¯• {next_model} æ¨¡åž‹...")
                try:
                    return setup_whisper_model(next_model)
                except:
                    continue
        
        # å¦‚æžœæ‰€æœ‰å¤‡é€‰éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise e

def auto_select_model(video_path, user_model_size='medium'):
    """æ ¹æ®è§†é¢‘æ—¶é•¿è‡ªåŠ¨é€‰æ‹©æ¨¡åž‹å¤§å°"""
    try:
        import subprocess
        
        # èŽ·å–è§†é¢‘æ—¶é•¿
        result = subprocess.run([
            'ffprobe', '-v', 'error', '-show_entries', 
            'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', video_path
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            duration = float(result.stdout.strip())
            
            # æ ¹æ®æ—¶é•¿æŽ¨èæ¨¡åž‹
            if duration <= 300:  # 5åˆ†é’Ÿä»¥å†…
                recommended = 'small'
            elif duration <= 1800:  # 30åˆ†é’Ÿä»¥å†…
                recommended = 'medium'
            else:  # è¶…è¿‡30åˆ†é’Ÿ
                recommended = 'large'
            
            # å¦‚æžœç”¨æˆ·æŒ‡å®šçš„æ¨¡åž‹æ¯”æŽ¨èçš„å°ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„
            model_sizes = ['tiny', 'base', 'small', 'medium', 'large']
            user_index = model_sizes.index(user_model_size) if user_model_size in model_sizes else 2
            recommended_index = model_sizes.index(recommended) if recommended in model_sizes else 2
            
            if user_index < recommended_index:
                print(f"âš ï¸  è§†é¢‘æ—¶é•¿ {duration:.1f}ç§’ å»ºè®®ä½¿ç”¨ {recommended} æ¨¡åž‹ï¼Œä½†å°†ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ {user_model_size} æ¨¡åž‹")
                return user_model_size
            else:
                print(f"ðŸ“Š è§†é¢‘æ—¶é•¿ {duration:.1f}ç§’ï¼Œè‡ªåŠ¨é€‰æ‹© {recommended} æ¨¡åž‹")
                return recommended
        
    except Exception as e:
        print(f"âš ï¸  æ— æ³•èŽ·å–è§†é¢‘æ—¶é•¿ï¼Œä½¿ç”¨é»˜è®¤æ¨¡åž‹: {e}")
    
    return user_model_size
